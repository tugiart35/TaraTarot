#!/usr/bin/env python3
"""
Ä°lk 10 String Test - Nested YapÄ± KorumasÄ±
"""

import json
from pathlib import Path
from transformers import MarianTokenizer, MarianMTModel
import torch
import re
from typing import Dict, Tuple

# Placeholder koruma
PLACEHOLDER_RE = re.compile(r"(\{\{.*?\}\}|%\w|%\{.*?\}|\{[0-9]+\}|\{[a-zA-Z_][a-zA-Z0-9_]*\}|<[^>]+>)")

def protect_placeholders(text: str) -> Tuple[str, Dict[str, str]]:
    tokens = {}
    def replace_placeholder(match):
        key = f"__PH_{len(tokens)}__"
        tokens[key] = match.group(0)
        return key
    protected = PLACEHOLDER_RE.sub(replace_placeholder, text)
    return protected, tokens

def restore_placeholders(text: str, tokens: Dict[str, str]) -> str:
    for placeholder_key, original_value in tokens.items():
        text = text.replace(placeholder_key, original_value)
    return text

def collect_strings(obj, path=()):
    """Nested yapÄ±yÄ± koruyarak string'leri topla"""
    out = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            out += collect_strings(v, path + (k,))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            out += collect_strings(v, path + (i,))
    elif isinstance(obj, str):
        out.append((path, obj))
    return out

def set_by_path(obj, path, value):
    """Path'e gÃ¶re deÄŸeri yerleÅŸtir"""
    cur = obj
    for p in path[:-1]:
        cur = cur[p]
    cur[path[-1]] = value

print("="*70)
print("ðŸ§ª Ä°LK 10 STRING TEST - NESTED YAPI KORUNMASI")
print("="*70)

# tr.json'dan ilk 10 string'i al
print("\nðŸ“– tr.json okunuyor...")
with open('messages/tr.json', 'r', encoding='utf-8') as f:
    tr_data = json.load(f)

all_strings = collect_strings(tr_data)
test_strings = all_strings[:10]

print(f"âœ… Toplam {len(all_strings):,} string bulundu")
print(f"ðŸ§ª Ä°lk 10 string test edilecek\n")

# Model yÃ¼kle
print("ðŸ“¦ MarianMT model yÃ¼kleniyor...")
tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-tr-en')
model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-tr-en')
print("âœ… Model hazÄ±r!\n")

print("="*70)
print("TÃœRKÃ‡E â†’ Ä°NGÄ°LÄ°ZCE Ã‡EVÄ°RÄ° TESTÄ°")
print("="*70)

# Ã‡eviriyi yap
import copy
translated_data = copy.deepcopy(tr_data)

for idx, (path, original_text) in enumerate(test_strings, 1):
    print(f"\n{idx}. TEST")
    print("-" * 70)
    
    # Path'i gÃ¶ster
    path_str = " â†’ ".join(str(p) for p in path)
    print(f"ðŸ“ Path: {path_str}")
    print(f"ðŸ‡¹ðŸ‡· TR : {original_text[:80]}...")
    
    # Placeholder koru
    protected_text, placeholders = protect_placeholders(original_text)
    
    # Ã‡evir
    inputs = tokenizer(protected_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model.generate(**inputs)
    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Placeholder'larÄ± geri yÃ¼kle
    final_text = restore_placeholders(translated, placeholders)
    
    print(f"ðŸ‡¬ðŸ‡§ EN : {final_text[:80]}...")
    
    if placeholders:
        print(f"ðŸ›¡ï¸  Korunan: {list(placeholders.values())}")
    
    # Template'e yerleÅŸtir
    set_by_path(translated_data, path, final_text)

print("\n" + "="*70)
print("âœ… Ã‡EVÄ°RÄ° TAMAMLANDI!")
print("="*70)

# SonuÃ§ yapÄ±sÄ±nÄ± kontrol et
print("\nðŸ“Š NESTED YAPI KONTROLÃœ:")
print("-" * 70)

# Ä°lk path'i kontrol et
first_path, _ = test_strings[0]
print(f"\n1ï¸âƒ£  Ä°lk string path: {first_path}")

# Nested eriÅŸim
current = translated_data
for p in first_path:
    current = current[p]

print(f"   DeÄŸer: {current[:80]}...")
print(f"   âœ… NESTED yapÄ± korundu!")

# JSON formatÄ±nÄ± gÃ¶ster
print("\nðŸ“„ Ä°LK 3 KEY'Ä°N JSON YAPISI:")
print("-" * 70)

# Ä°lk top-level key'i al
first_top_key = list(translated_data.keys())[0]
sample = {first_top_key: translated_data[first_top_key]}

# Sadece ilk 3 nested item gÃ¶ster
def limit_dict(d, max_items=3, depth=0):
    if depth > 3:  # Max depth
        return "..."
    if isinstance(d, dict):
        limited = {}
        for i, (k, v) in enumerate(d.items()):
            if i >= max_items:
                limited["..."] = f"(+{len(d) - max_items} more)"
                break
            limited[k] = limit_dict(v, max_items, depth + 1)
        return limited
    elif isinstance(d, list):
        return [limit_dict(item, max_items, depth + 1) for item in d[:max_items]]
    return d

limited_sample = limit_dict(sample, max_items=2)
print(json.dumps(limited_sample, ensure_ascii=False, indent=2))

print("\n" + "="*70)
print("âœ… TEST BAÅžARILI!")
print("="*70)
print("\nðŸ’¡ SONUÃ‡:")
print("  âœ… Nested yapÄ± korundu")
print("  âœ… Placeholder'lar korundu")
print("  âœ… Path-based Ã§eviri Ã§alÄ±ÅŸÄ±yor")
print("  âœ… JSON yapÄ±sÄ± bozulmadÄ±")
print("\nðŸš€ Script production'a hazÄ±r!")
print("="*70)


